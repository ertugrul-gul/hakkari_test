import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from statsmodels.tsa.api import VAR
from statsmodels.tsa.statespace.sarimax import SARIMAX
from prophet import Prophet
import os
import pickle  # Prophet modelini kaydetmek iÃ§in kullanacaÄŸÄ±z



df = pd.read_csv("hakkari_0.csv")
# valid_time'Ä± datetime formatÄ±na Ã§evir
df["valid_time"] = pd.to_datetime(df["valid_time"], errors="coerce")
df.info()

# 3. Eksik veri kontrolÃ¼
print("\nğŸ”¹ Eksik Veri KontrolÃ¼:")
print(df.isnull().sum())

# 4. FarklÄ± koordinat sayÄ±sÄ±nÄ± kontrol et
print("\nğŸ”¹ Toplam FarklÄ± Koordinat SayÄ±sÄ±:")
print(df.groupby(["lat", "lon"]).size().reset_index().shape[0])

# 5. FarklÄ± tarihler ve eksik veri olup olmadÄ±ÄŸÄ±nÄ± incele
print("\nğŸ”¹ Toplam FarklÄ± Tarih SayÄ±sÄ±:")
print(df["valid_time"].nunique())

# 6. Her koordinat iÃ§in tÃ¼m tarihlerde eksiksiz veri olup olmadÄ±ÄŸÄ±nÄ± kontrol et
print("\nğŸ”¹ Koordinat-Tarih BazÄ±nda Eksik Veri SayÄ±sÄ±:")
missing_values = df.groupby(["lat", "lon", "valid_time"]).size().unstack(fill_value=0)
print(missing_values.apply(lambda x: x.isnull().sum(), axis=1))

# 7. Ä°lk birkaÃ§ satÄ±rÄ± gÃ¶rÃ¼ntÃ¼le
print("\nğŸ”¹ Ä°lk 5 SatÄ±r:")
print(df.head())



# 1. Tarih bilgilerini yÄ±l, ay, gÃ¼n olarak ayÄ±ralÄ±m
df["year"] = df["valid_time"].dt.year
df["month"] = df["valid_time"].dt.month
df["day"] = df["valid_time"].dt.day

# 2. GiriÅŸ deÄŸiÅŸkenlerini belirleyelim (BaÄŸÄ±msÄ±z deÄŸiÅŸkenler X)
X = df[["year", "month", "day", "lat", "lon", "sp", "u10", "v10"]]

# 3. Ã‡Ä±kÄ±ÅŸ deÄŸiÅŸkenlerini belirleyelim (BaÄŸÄ±mlÄ± deÄŸiÅŸkenler Y)
y = df[["t2m", "tp"]]

# 4. Veriyi eÄŸitim ve test setlerine ayÄ±ralÄ±m (%80 eÄŸitim, %20 test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)

# 5. Veriyi Ã¶lÃ§ekleyelim (StandardScaler ile normalleÅŸtirme yapalÄ±m)
scaler_X = StandardScaler()
scaler_Y = StandardScaler()

X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

y_train_scaled = scaler_Y.fit_transform(y_train)
y_test_scaled = scaler_Y.transform(y_test)

print("ğŸ”¹ Veri baÅŸarÄ±yla iÅŸlendi ve Ã¶lÃ§eklendi!")
print(f"X_train boyutu: {X_train.shape}, y_train boyutu: {y_train.shape}")
print(f"X_test boyutu: {X_test.shape}, y_test boyutu: {y_test.shape}")




# 1ï¸âƒ£ XGBoost Modeli EÄŸitme
xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
xgb_model.fit(X_train_scaled, y_train)

# 2ï¸âƒ£ Random Forest Modeli EÄŸitme
rf_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
rf_model.fit(X_train_scaled, y_train)

# 3ï¸âƒ£ Tahminleri AlalÄ±m
y_pred_xgb = xgb_model.predict(X_test_scaled)
y_pred_rf = rf_model.predict(X_test_scaled)

# 4ï¸âƒ£ Model PerformanslarÄ±nÄ± Ã–lÃ§elim
def evaluate_model(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    print(f"\nğŸ”¹ {model_name} PerformansÄ±:")
    print(f"ğŸ“Œ MAE: {mae}")
    print(f"ğŸ“Œ RMSE: {rmse}")

evaluate_model(y_test, y_pred_xgb, "XGBoost")
evaluate_model(y_test, y_pred_rf, "Random Forest")



# 1ï¸âƒ£ LSTM Ä°Ã§in Veriyi 3D Formata Getir
X_train_lstm = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_lstm = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# 2ï¸âƒ£ LSTM Modelini OluÅŸtur
lstm_model = Sequential([
    LSTM(50, activation='relu', return_sequences=True, input_shape=(1, X_train_scaled.shape[1])),
    Dropout(0.2),
    LSTM(25, activation='relu'),
    Dropout(0.2),
    Dense(2)  # 2 Ã‡Ä±kÄ±ÅŸ: t2m (sÄ±caklÄ±k) ve tp (yaÄŸÄ±ÅŸ)
])

# 3ï¸âƒ£ Modeli Derle
lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# 4ï¸âƒ£ Modeli EÄŸit
lstm_model.fit(X_train_lstm, y_train_scaled, epochs=20, batch_size=32, validation_data=(X_test_lstm, y_test_scaled), verbose=1)

# 5ï¸âƒ£ Tahminleri Al
y_pred_lstm_scaled = lstm_model.predict(X_test_lstm)

# 6ï¸âƒ£ Ã–lÃ§eklendirilmiÅŸ Tahminleri Geri DÃ¶nÃ¼ÅŸtÃ¼r
y_pred_lstm = scaler_Y.inverse_transform(y_pred_lstm_scaled)

# 7ï¸âƒ£ Model PerformansÄ±nÄ± DeÄŸerlendir
evaluate_model(y_test, y_pred_lstm, "LSTM")

# 1ï¸âƒ£ MLP Modelini OluÅŸtur
mlp_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dense(32, activation='relu'),
    Dense(2)  # 2 Ã‡Ä±kÄ±ÅŸ: t2m (sÄ±caklÄ±k) ve tp (yaÄŸÄ±ÅŸ)
])

# 2ï¸âƒ£ Modeli Derle
mlp_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# 3ï¸âƒ£ Modeli EÄŸit
mlp_model.fit(X_train_scaled, y_train_scaled, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test_scaled), verbose=1)

# 4ï¸âƒ£ Tahminleri Al
y_pred_mlp_scaled = mlp_model.predict(X_test_scaled)

# 5ï¸âƒ£ Ã–lÃ§eklendirilmiÅŸ Tahminleri Geri DÃ¶nÃ¼ÅŸtÃ¼r
y_pred_mlp = scaler_Y.inverse_transform(y_pred_mlp_scaled)

# 6ï¸âƒ£ Model PerformansÄ±nÄ± DeÄŸerlendir
evaluate_model(y_test, y_pred_mlp, "MLP")



# 1ï¸âƒ£ VAR Ä°Ã§in Veriyi HazÄ±rlama (Tarih bazÄ±nda sÄ±ralama)
df_var = df[["valid_time", "t2m", "tp", "sp", "u10", "v10"]].copy()
df_var.set_index("valid_time", inplace=True)
df_var.sort_index(inplace=True)

# 2ï¸âƒ£ VAR Modeli EÄŸitme
var_model = VAR(df_var)
var_results = var_model.fit(maxlags=5)

# 3ï¸âƒ£ Tahmin Yapma (Test seti kadar ileriye tahmin yap)
var_forecast = var_results.forecast(df_var.values[-5:], steps=len(y_test))

# 4ï¸âƒ£ GerÃ§ek ve Tahmin Edilen DeÄŸerleri KarÅŸÄ±laÅŸtÄ±rma
y_pred_var = var_forecast[:, :2]  # Ä°lk iki sÃ¼tun t2m ve tp

# 5ï¸âƒ£ Model PerformansÄ±nÄ± DeÄŸerlendir
evaluate_model(y_test, y_pred_var, "VAR")



# Tarih sÃ¼tununu datetime formatÄ±na Ã§evirip index olarak ayarla
df_var.index = pd.DatetimeIndex(df_var.index).to_period('D')  # GÃ¼nlÃ¼k veri olduÄŸunu belirtiyoruz




# 1ï¸âƒ£ SARIMA Modeli EÄŸitme (Sadece sÄ±caklÄ±k tahmini yapacak)
sarima_model = SARIMAX(df_var["t2m"], order=(1,1,1), seasonal_order=(1,1,1,12), freq='D')
sarima_results = sarima_model.fit()

# 2ï¸âƒ£ Tahmin Yapma
y_pred_sarima = sarima_results.forecast(steps=len(y_test))

# 3ï¸âƒ£ Model PerformansÄ±nÄ± DeÄŸerlendir
evaluate_model(y_test["t2m"], y_pred_sarima, "SARIMA")



# 1ï¸âƒ£ Prophet Modeli Ä°Ã§in Veriyi HazÄ±rlayalÄ±m
df_prophet = df_var.reset_index()[["valid_time", "t2m"]]  # Prophet iÃ§in sadece tarih ve sÄ±caklÄ±k
df_prophet.columns = ["ds", "y"]  # Prophet formatÄ±: Tarih (ds), DeÄŸer (y)

# 2ï¸âƒ£ Prophet Modelini Ã–nceden KaydedilmiÅŸ Dosya Var mÄ± Kontrol Et
prophet_model_path = "prophet_model.pkl"

if os.path.exists(prophet_model_path):
    # KaydedilmiÅŸ modeli yÃ¼kle
    with open(prophet_model_path, "rb") as f:
        prophet_model = pickle.load(f)
    print("ğŸ”¹ Prophet modeli Ã¶nceden eÄŸitilmiÅŸ, kaydedilmiÅŸ model yÃ¼kleniyor...")
else:
    # Yeni Prophet modeli oluÅŸtur
    prophet_model = Prophet(yearly_seasonality=True, daily_seasonality=False, weekly_seasonality=True)

    # Modeli eÄŸit
    prophet_model.fit(df_prophet)

    # Modeli kaydet
    with open(prophet_model_path, "wb") as f:
        pickle.dump(prophet_model, f)

    print("âœ… Prophet modeli eÄŸitildi ve kaydedildi!")

# 3ï¸âƒ£ Tahmin Yapma
future_dates = prophet_model.make_future_dataframe(periods=len(y_test))
forecast = prophet_model.predict(future_dates)

# 4ï¸âƒ£ Test setine denk gelen tahminleri alalÄ±m
y_pred_prophet = forecast.iloc[-len(y_test):]["yhat"].values

# 5ï¸âƒ£ Model PerformansÄ±nÄ± DeÄŸerlendir
evaluate_model(y_test["t2m"], y_pred_prophet, "Prophet")
from prophet import Prophet
import os
import pickle  # Prophet modelini kaydetmek iÃ§in kullanacaÄŸÄ±z

# 1ï¸âƒ£ Prophet Modeli Ä°Ã§in Veriyi HazÄ±rlayalÄ±m
df_prophet = df_var.reset_index()[["valid_time", "t2m"]]  # Prophet iÃ§in sadece tarih ve sÄ±caklÄ±k
df_prophet.columns = ["ds", "y"]  # Prophet formatÄ±: Tarih (ds), DeÄŸer (y)

# 2ï¸âƒ£ Prophet Modelini Ã–nceden KaydedilmiÅŸ Dosya Var mÄ± Kontrol Et
prophet_model_path = "prophet_model.pkl"

if os.path.exists(prophet_model_path):
    # KaydedilmiÅŸ modeli yÃ¼kle
    with open(prophet_model_path, "rb") as f:
        prophet_model = pickle.load(f)
    print("ğŸ”¹ Prophet modeli Ã¶nceden eÄŸitilmiÅŸ, kaydedilmiÅŸ model yÃ¼kleniyor...")
else:
    # Yeni Prophet modeli oluÅŸtur
    prophet_model = Prophet(yearly_seasonality=True, daily_seasonality=False, weekly_seasonality=True)

    # Modeli eÄŸit
    prophet_model.fit(df_prophet)

    # Modeli kaydet
    with open(prophet_model_path, "wb") as f:
        pickle.dump(prophet_model, f)

    print("âœ… Prophet modeli eÄŸitildi ve kaydedildi!")

# 3ï¸âƒ£ Tahmin Yapma
future_dates = prophet_model.make_future_dataframe(periods=len(y_test))
forecast = prophet_model.predict(future_dates)

# 4ï¸âƒ£ Test setine denk gelen tahminleri alalÄ±m
y_pred_prophet = forecast.iloc[-len(y_test):]["yhat"].values

# 5ï¸âƒ£ Model PerformansÄ±nÄ± DeÄŸerlendir
evaluate_model(y_test["t2m"], y_pred_prophet, "Prophet")

